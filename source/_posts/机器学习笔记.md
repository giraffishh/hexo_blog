---
banner_img: 'https://img.picgo.net/2025/03/28/25-03-28-17431514678427b2138c37794ab42.webp'
index_img: 'https://img.picgo.net/2025/03/28/25-03-28-1743150706296586d07573c738663.webp'
title: 机器学习笔记
tags:
  - 机器学习
categories:
  - 学习笔记
comments: true
abbrlink: b1b54fd
date: 2025-03-28 16:32:50
updated: 2025-03-30 20:04:09
---


[**课程主页**](https://www.coursera.org/specializations/machine-learning-introduction/?utm_medium=coursera&utm_source=home-page&utm_campaign=mlslaunch2022IN)
[**配套练习代码**](https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera)


## 1. 监督学习

监督学习是已经知道数据的label，例如预测房价问题，给出了房子的面积和价格

* 回归问题是预测连续值的输出，例如预测房价

![](https://img.picgo.net/2025/03/28/25-03-28-1743143619246414111475a027645.png)

* 分类问题是预测离散值输出，例如判断肿瘤是良性还是恶性

![](https://img.picgo.net/2025/03/28/25-03-28-1743143695127ad770c735a0fc083.webp)

## 2. 无监督学习

无监督学习是不知道数据具体的含义，比如给定一些数据但不知道它们具体的信息，对于分类问题无监督学习可以得到多个不同的聚类，从而实现预测的功能

![](https://img.picgo.net/2025/03/28/25-03-28-17431438061779e4e3706e0702341.webp)

## 3. 线性回归

### 3.1 代价函数（Cost Function)

最小二乘法

![](https://s1.imagehub.cc/images/2025/03/28/69b495174b9398e22194dcf74f999504.webp)

![](https://img.picgo.net/2025/03/28/25-03-28-1743145270302c696bd8423535a9d.webp)

### 3.2 梯度下降（Gradient Descent）

梯度下降，首先为每个参数赋一个初值，通过代价函数的梯度，然后不断地调整参数，最终得到一个局部最优解。初值的不同可能会得到两个不同的结果，即梯度下降不一定得到全局最优解

![](https://img.picgo.net/2025/03/28/25-03-28-174314479530291a8cd37389b8d2c.webp)

梯度下降在具体的执行时，每一次更新需要同时更新所有的参数。

![](https://img.picgo.net/2025/03/28/25-03-28-1743145542315d477ee9cf96be051.webp)

![](https://img.picgo.net/2025/03/28/25-03-28-1743145895067fedf5b6eb8369fdf.webp)

梯度下降效果图示

![](https://s1.imagehub.cc/images/2025/03/16/b3660e0a2d9d4dfc4d4e199908b92671.png)

梯度下降过程容易出现局部最优解

![](https://img.picgo.net/2025/03/28/25-03-28-17431461519203882e40b5917ae6d.webp)

但是线性回归的代价函数往往是凸函数，总能收敛到全局最优解

![](https://img.picgo.net/2025/03/28/25-03-28-17431462243244c0997b3075442e3.webp)

**学习曲线：**

![](https://img.picgo.net/2025/03/29/25-03-29-1743233763258d724c0d3c06f17a7.webp)

**学习率:** 过小下降慢，过大可能无法收敛

![](https://img.picgo.net/2025/03/28/25-03-28-17431464178914c3c8e41debe543c.webp)

寻找最佳学习率

![](https://img.picgo.net/2025/03/29/25-03-29-1743234261143a42e768db385b2ff.webp)

**多元梯度下降与向量化**

通常问题都会涉及到多个变量，例如房屋价格预测就包括，面积、房间个数、楼层、价格等

![](https://img.picgo.net/2025/03/29/25-03-29-1743232186021a8e30f67e5c2cec3.webp)

而向量化进行矩阵运算可以显著提高运算效率

![](https://img.picgo.net/2025/03/29/25-03-29-1743232285309ef96d4d895379443.webp)

$$
\mathbf{X} = 
\begin{pmatrix}
 x^{(0)}_0 & x^{(0)}_1 & \cdots & x^{(0)}_{n-1} \\ 
 x^{(1)}_0 & x^{(1)}_1 & \cdots & x^{(1)}_{n-1} \\
 \cdots \\
 x^{(m-1)}_0 & x^{(m-1)}_1 & \cdots & x^{(m-1)}_{n-1} 
\end{pmatrix}
$$

**Notation:**

* $\mathbf{x}^{(i)}$ is vector containing example i. $\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \cdots,x^{(i)}_{n-1})$
* $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element. 

$$
\mathbf{w} = \begin{pmatrix}
w_0 \\ 
w_1 \\
\cdots\\
w_{n-1}
\end{pmatrix}
$$

$$
f_{\mathbf{w},b}(\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b = \mathbf{w} \cdot \mathbf{x} + b
$$

### 3.3 正规方程（Normal Equation）

正规方程是一种**解析解**方法，可直接求解线性回归的最优参数，无需迭代优化。

假设线性回归模型为：
$$
y = XW + \varepsilon
$$
其中：
- $X$ 是特征矩阵（包含偏置项 $x_0 = 1$）。
- $W$ 是待求参数向量。
- $y$ 是目标值向量。
- $\varepsilon$ 是误差项。

最优参数 $W$ 由正规方程计算：
$$
W = (X^T X)^{-1} X^T y
$$
其中：
- $X^T X$ 是一个 **正规矩阵**（为了求逆转为方阵，正规矩阵可以用酉矩阵对角化）
- $(X^T X)^{-1}$ 是它的逆矩阵（如果可逆）
- $X^T y$ 是特征与目标值的内积

正规方程求解方法与梯度下降对比：

![](https://img.picgo.net/2025/03/29/25-03-29-17432363753406b71859f8f29bd69.webp)

> 正规方程只能求解线性回归，且计算 $(X^T X)^{-1}$ 需要 $O(n^3)$ 复杂度，计算成本高

### 3.4 特征缩放

多个变量的度量不同，数字之间相差的大小也不同，如果可以将所有的特征变量缩放到大致相同范围，这样会减少梯度算法的迭代

特征缩放不一定非要落到[-1，1]之间，只要数据足够接近就可以

![](https://img.picgo.net/2025/03/29/25-03-29-17432331170437198a4025fc9d77a.webp)

**法一：均值归一化**

![](https://img.picgo.net/2025/03/29/25-03-29-17432331710286dab88e5ee871889.webp)

**法二：Z-score 标准化**

![](https://img.picgo.net/2025/03/29/25-03-29-1743233272396157041f20d1c1c6b.webp)

### 3.5 特征工程

通过转换和组合原始特征，形成新的特征，能使学习算法做出更准确的预测

![](https://img.picgo.net/2025/03/29/25-03-29-1743234876149af5a3aa0ed89f70d.webp)

### 3.6 多项式回归

![](https://img.picgo.net/2025/03/29/25-03-29-17432394184991cc882f35e5f1cc6.webp)

## 4. 逻辑回归

### 4.1 激活函数（Sigmoid Function）

![](https://img.picgo.net/2025/03/30/25-03-30-1743304936154b96adf121e19d3b8.webp)

### 4.2 决策边界

$\begin{array}{rrr}\overrightarrow{\mathrm{w}} \cdot \overrightarrow{\mathrm{x}}+b \geq 0 & \hat{y}=1 &  \\ \overrightarrow{\mathrm{w}} \cdot \overrightarrow{\mathrm{x}}+b<0  & \hat{y}=0\end{array}$

![](https://img.picgo.net/2025/03/30/25-03-30-17433060425694d198e01b78f6338.webp)

非线性的决策边界同理

![](https://img.picgo.net/2025/03/30/25-03-30-174330610434850110bc9102955be.webp)

### 4.3 代价函数

对于`Sigmoid函数`，如果使用线性回归中的代价函数$J(w, b)=\frac{1}{2 m} \sum_{i=0}^{m-1}\left(f_{w, b}\left(x^{(i)}\right)-y^{(i)}\right)^2$，将会得到一个非凸函数，使得无法使用梯度下降收敛到全局最优解

![](https://img.picgo.net/2025/03/30/25-03-30-1743324215431c1b36e37f76cc29e.webp)

![](https://img.picgo.net/2025/03/30/25-03-30-174332477258962e03f447ca72f21.webp)

逻辑回归中一般使用**对数函数**作为损失函数

![](https://img.picgo.net/2025/03/30/25-03-30-1743324240846e3baab1bd31f7771.webp)

效果如图

![](https://img.picgo.net/2025/03/30/25-03-30-1743324826711101bd8ccbdd99acd.webp)

$$
loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = \begin{cases}
    - \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) & \text{if $y^{(i)}=1$}\\
    - \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) & \text{if $y^{(i)}=0$}
  \end{cases}
$$

可以将其合并为一条式子

$$
loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
$$

将其代入**代价函数**得到
$$
\begin{aligned}
& J(\overrightarrow{\mathrm{w}}, b)=\frac{1}{m} \sum_{i=1}^m{L\left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right), y^{(i)}\right)} \\
&=-\frac{1}{m} \sum_{i=1}^m\left[y^{(i)} \log \left(f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-f_{\overrightarrow{\mathrm{w}}, b}\left(\overrightarrow{\mathrm{x}}^{(i)}\right)\right)\right]
\end{aligned}
$$

### 4.4 梯度下降

与线性回归中的梯度下降同理

![](https://img.picgo.net/2025/03/30/25-03-30-17433263140992730e30a3acbd8e3.webp)

[推导过程参考](https://blog.giraffish.site/)

![](https://img.picgo.net/2025/03/30/25-03-30-174332706312215d24379298f641f.webp)

效果如图

![](https://img.picgo.net/2025/03/30/25-03-30-174333571497148031cd7deff46cc.gif)
